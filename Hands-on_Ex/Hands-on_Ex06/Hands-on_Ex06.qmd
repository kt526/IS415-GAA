---
title: "Hands-on Exercise 6: Geographical Segmentation with Spatially Constrained Clustering Techniques"
execute:
  warning: false
date: 03/01/2024
code-annotations: hover
toc-depth: 3
---

# 1.0 Introduction

## 1.1 Getting Started

In this hands-on exercise, you will gain hands-on experience on how to delineate homogeneous region by using geographically referenced multivariate data. There are two major analysis, namely:

-   Hierarchical cluster analysis; and

-   Spatially constrained cluster analysis.

## 1.2 The Analytical Question

In geobusiness and spatial policy, it is a common practice to delineate the market or planning area into homogeneous regions by using multivariate data. In this hands-on exercise, we are interested to delineate Shan State, Myanmar into homogeneous regions by using multiple Information and Communication technology (ICT) measures, namely: Radio, Television, Land line phone, Mobile phone, Computer, and Internet at home.

## 1.3 Installing and loading R packages

```{r}
pacman::p_load(rgdal, spdep, tmap, sf, ClustGeo, 
               ggpubr, cluster, factoextra, NbClust,
               heatmaply, corrplot, psych, tidyverse, GGally)
```

# 2.0 Data Acquisition

We will be using 2 datasets in this exercise:

-   Myanmar Township Boundary Data

-   Shan-ICT.csv

# 3.0 Geospatial Data Handling

We will be using the *st_read()* from **sf** package to import the data into RStudio.

## 3.1 Importing Geospatial Data

The `Myanmar Township Boundary` GIS data is in ESRI shapefile format. We will import this data into the R environment using the [*st_read()*](https://www.rdocumentation.org/packages/sf/versions/0.7-2/topics/st_read) function of **sf**.

```{r}
shan_sf <- st_read(dsn = "data/geospatial", 
                   layer = "myanmar_township_boundaries") %>%
  filter(ST %in% c("Shan (East)", "Shan (North)", "Shan (South)")) %>%
  select(c(2:7))
```

## 3.2 Importing Aspatial Data

The `Shan-ICT` file is in .csv format. We will be import it using *read_csv* function of **readr** package.

```{r}
ict <- read_csv("data/aspatial/Shan-ICT.csv")
```

## 3.3 Data Preparation

### 3.3.1 Derive penetration rate of each ICT variable

The unit of measurement of the values are number of household. Using these values directly will be bias by the underlying total number of households. In general, the townships with relatively higher total number of households will also have higher number of households owning radio, TV, etc.

```{r}
ict_derived <- ict %>%
  mutate(`RADIO_PR` = `Radio`/`Total households`*1000) %>%
  mutate(`TV_PR` = `Television`/`Total households`*1000) %>%
  mutate(`LLPHONE_PR` = `Land line phone`/`Total households`*1000) %>%
  mutate(`MPHONE_PR` = `Mobile phone`/`Total households`*1000) %>%
  mutate(`COMPUTER_PR` = `Computer`/`Total households`*1000) %>%
  mutate(`INTERNET_PR` = `Internet at home`/`Total households`*1000) %>%
  rename(`DT_PCODE` =`District Pcode`,`DT`=`District Name`,
         `TS_PCODE`=`Township Pcode`, `TS`=`Township Name`,
         `TT_HOUSEHOLDS`=`Total households`,
         `RADIO`=`Radio`, `TV`=`Television`, 
         `LLPHONE`=`Land line phone`, `MPHONE`=`Mobile phone`,
         `COMPUTER`=`Computer`, `INTERNET`=`Internet at home`) 
```

We can review the summary statistics of the newly derived penetration rates using the *summary()* function.

```{r}
summary(ict_derived)
```

# 4.0 Exploratory Data Analysis  (EDA)

## 4.1 Statistical graphics

We can plot the distribution of the variables (i.e. Number of households with radio) by using appropriate Exploratory Data Analysis (EDA).

### 4.1.1 Histograms

Histogram is useful to identify the overall distribution of the data values (i.e. left skew, right skew or normal distribution)

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")
```

### 4.1.2 Statistical graphics - Multiple Histograms

```{r}
radio <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

tv <- ggplot(data=ict_derived, 
             aes(x= `TV_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

llphone <- ggplot(data=ict_derived, 
             aes(x= `LLPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

mphone <- ggplot(data=ict_derived, 
             aes(x= `MPHONE_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

computer <- ggplot(data=ict_derived, 
             aes(x= `COMPUTER_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

internet <- ggplot(data=ict_derived, 
             aes(x= `INTERNET_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue")

ggarrange(radio, tv, llphone, mphone, computer, internet, 
          ncol = 3, 
          nrow = 2)
```

::: callout-note
## Note

The [ggarrange()](https://rpkgs.datanovia.com/ggpubr/reference/ggarrange.html) function of [**ggpubr**](https://rpkgs.datanovia.com/ggpubr/) package is used to group these histograms together.
:::

### 4.1.3 Statistical graphics - Boxplot

Boxplot is useful to detect if there are outliers.

```{r}
ggplot(data=ict_derived, 
       aes(x=`RADIO`)) +
  geom_boxplot(color="black", 
               fill="light blue")
```

## 4.2 Choropleth Map

### 4.2.1 Preparing the data for choropleth map

Before we can prepare the choropleth map, we need to combine both the geospatial data object (i.e. *shan_sf*) and aspatial data.frame object (i.e. *ict_derived*) into one using the left_join() function of dplyr package.

```{r}
shan_sf <- left_join(shan_sf, 
                     ict_derived, by=c("TS_PCODE"="TS_PCODE"))
  
# write_rds(shan_sf, "data/rds/shan_sf.rds")
shan_sf <- read_rds("data/rds/shan_sf.rds")
```

::: callout-note
## Note

The unique identifier used to join both data objects is `TS_PCODE`.
:::

### 4.2.2 Plotting choropleth map

```{r}
qtm(shan_sf, "RADIO_PR")
```

::: callout-note
## Note

The distribution shown in the choropleth map above are bias to the underlying total number of households at the townships
:::

When we compare the two choropleth maps below, we can tell the biasness.

```{r}
TT_HOUSEHOLDS.map <- tm_shape(shan_sf) + 
  tm_fill(col = "TT_HOUSEHOLDS",
          n = 5,
          style = "jenks", 
          title = "Total households") + 
  tm_borders(alpha = 0.5) 

RADIO.map <- tm_shape(shan_sf) + 
  tm_fill(col = "RADIO",
          n = 5,
          style = "jenks",
          title = "Number Radio ") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(TT_HOUSEHOLDS.map, RADIO.map,
             asp=NA, ncol=2)
```

::: callout-note
## Note

Notice that the choropleth maps above clearly show that townships with relatively larger number ot households are also showing relatively higher number of radio ownership.
:::

Now, let us plot the choropleth maps showing the distribution of total number of households and Radio penetration rate.

```{r}
tm_shape(shan_sf) +
    tm_polygons(c("TT_HOUSEHOLDS", "RADIO_PR"),
                style="jenks") +
    tm_facets(sync = TRUE, ncol = 2) +
  tm_legend(legend.position = c("right", "bottom"))+
  tm_layout(outer.margins=0, asp=0)
```

# 5.0 Correlation Analysis

Before we perform cluster analysis, it is important for us to ensure that the cluster variables are not highly correlated.

```{r}
cluster_vars.cor = cor(ict_derived[,12:17])
corrplot.mixed(cluster_vars.cor,
         lower = "ellipse", 
               upper = "number",
               tl.pos = "lt",
               diag = "l",
               tl.col = "black")
```

The correlation plot above shows that COMPUTER_PR and INTERNET_PR are highly correlated. This suggest that only one of them should be used in the cluster analysis instead of both.

# 6.0 Hierarchy Cluster Analysis

In this section, we will be performing hierarchical cluster analysis. The analysis consists of four major steps:

1.  Extracting clustering variables
2.  Data Standardisation
3.  Computing proximity matric
4.  Computing hierarchical clustering

## 6.1 Extracting clustering variables

Firstly, let us extract the variables that will be used for clustering analysis from the `shan_sf` simple feature object into data.frame.

```{r}
cluster_vars <- shan_sf %>%
  st_set_geometry(NULL) %>%
  select("TS.x", "RADIO_PR", "TV_PR", "LLPHONE_PR", "MPHONE_PR", "COMPUTER_PR")
head(cluster_vars,10)
```

::: callout-note
## Note

Notice that the final clustering variables list does not include variable INTERNET_PR because it is highly correlated with variable COMPUTER_PR.
:::

```{r}
row.names(cluster_vars) <- cluster_vars$"TS.x"
head(cluster_vars,10)
```

```{r}
shan_ict <- select(cluster_vars, c(2:6))
head(shan_ict, 10)
```

## 6.2 Data Standardisation

In general, multiple variables will be used in cluster analysis. It is not unusual their values range are different. In order to avoid the cluster analysis result is baised to clustering variables with large values, it is useful to standardise the input variables before performing cluster analysis.

### 6.2.1 Min-Max standardisation

The *normalize()* of [*heatmaply*](https://cran.r-project.org/web/packages/heatmaply/) package is used to stadardisation the clustering variables by using Min-Max method. The *summary()* is then used to display the summary statistics of the standardised clustering variables.

```{r}
shan_ict.std <- normalize(shan_ict)
summary(shan_ict.std)
```

::: callout-note
## Note

Notice that the values range of the Min-max standardised clustering variables are 0-1 now.
:::

### 6.2.2 Z-score standardisation

Z-score standardisation can be performed easily by using [*scale()*](https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/scale) of Base R

```{r}
shan_ict.z <- scale(shan_ict)
describe(shan_ict.z)
```

::: callout-note
## Note

Notice the mean and standard deviation of the Z-score standardised clustering variables are 0 and 1 respectively.
:::

### 6.2.3 Visualising the standardised clustering variables

Beside reviewing the summary statistics of the standardised clustering variables, it is also a good practice to visualise their distribution graphical.

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_histogram(bins=20, 
                 color="black", 
                 fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

```{r}
r <- ggplot(data=ict_derived, 
             aes(x= `RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Raw values without standardisation")

shan_ict_s_df <- as.data.frame(shan_ict.std)
s <- ggplot(data=shan_ict_s_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Min-Max Standardisation")

shan_ict_z_df <- as.data.frame(shan_ict.z)
z <- ggplot(data=shan_ict_z_df, 
       aes(x=`RADIO_PR`)) +
  geom_density(color="black",
               fill="light blue") +
  ggtitle("Z-score Standardisation")

ggarrange(r, s, z,
          ncol = 3,
          nrow = 1)
```

## 6.3 Computing proximity matrix

We will be using the dist() function of R to calculate the proximity distance matrix.

```{r}
proxmat <- dist(shan_ict, method = 'euclidean')
```

```{r}
proxmat
```

::: callout-note
## Note

*dist()* supports six distance proximity calculations, they are: **euclidean, maximum, manhattan, canberra, binary and minkowski**. The default is *euclidean* proximity matrix.
:::

## 6.4 Computing hierarchical clustering

Next, we will use the hclust() function of R stats to compute hierarchical clustering.

```{r}
hclust_ward <- hclust(proxmat, method = 'ward.D')
```

```{r}
plot(hclust_ward, cex = 0.6)
```

::: callout-note
## Note

*hclust()* employed agglomeration method to compute the cluster. Eight clustering algorithms are supported, they are: ward.D, ward.D2, single, complete, average(UPGMA), mcquitty(WPGMA), median(WPGMC) and centroid(UPGMC).
:::

### 6.4.1 Selecting the optimal clustering algorithm

One of the challenge in performing hierarchical clustering is to identify stronger clustering structures. This issue can be solve by using agnes() function of cluster package.

```{r}
m <- c( "average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

ac <- function(x) {
  agnes(shan_ict, method = x)$ac
}

map_dbl(m, ac)
```

From the output above, we can see that the Ward's method provides the strongest clustering structure among the four methods assessed. Thus, for subsequent analysis, we will be using the Ward's method.

### 6.4.2 Determining Optimal Clusters

The second challenge faced in performing clustering analysis is to determine the optimal clusters to retain.

There are three commonly used methods to determine the optimal clusters, they are:

1.  Elbow method
2.  Average silhoutte method
3.  Gap statistic method

#### 6.4.2.1 Gap statistic method

The gap statistic method compares the total within intra-cluster variation for different values of k with their expected values under null reference distribution of the data. The estimate of the optimal clusters will be value that maximize the gap statistic (i.e., that yields the largest gap statistic). This means that the clustering structure is far away from the random uniform distribution of points.

```{r}
set.seed(12345)
gap_stat <- clusGap(shan_ict, 
                    FUN = hcut, 
                    nstart = 25, 
                    K.max = 10, 
                    B = 50)
# Print the result
print(gap_stat, method = "firstmax")
```

We can visualise the plot using *fviz_gap_stat()* of factoextra package.

```{r}
fviz_gap_stat(gap_stat)
```

With reference to the gap statistic graph above, the recommended number of cluster to retain is 1. However, it is not logical to retain only one cluster. By examine the gap statistic graph, the 6-cluster gives the largest gap statistic and should be the next best cluster to pick.

### 6.4.3 Interpreting Dendrograms

We can draw the dendrogram with a border around the selected clusters by using [*rect.hclust()*](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/rect.hclust.html) of R stats

```{r}
plot(hclust_ward, cex = 0.6)
rect.hclust(hclust_ward, 
            k = 6, 
            border = 2:5)
```

### 6.4.4 Creating visually driven hierarchical clustering analysis

#### 6.4.4.1 Transforming data frame into matrix

Before we start to build the heatmap, we have to ensure that the data is loaded as a data matrix instead of data frame. To do so, we have to transform shan_ict data frame into a data matrix.

```{r}
shan_ict_mat <- data.matrix(shan_ict)
```

#### 6.4.4.2 Plotting interactive cluster heatmap using heatmaply()

Next, we will use the heatmaply() function of the heatmaply package to build an interactive cluster heatmap.

```{r}
heatmaply(normalize(shan_ict_mat),
          Colv=NA,
          dist_method = "euclidean",
          hclust_method = "ward.D",
          seriate = "OLO",
          colors = Blues,
          k_row = 6,
          margins = c(NA,200,60,NA),
          fontsize_row = 4,
          fontsize_col = 5,
          main="Geographic Segmentation of Shan State by ICT indicators",
          xlab = "ICT Indicators",
          ylab = "Townships of Shan State")
```

### 6.4.5 Mapping clusters formed

With closed examination of the dendragram above, we have decided to retain six clusters.

[*cutree()*](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/cutree.html) of R Base will be used to derive a 6-cluster model.

```{r}
groups <- as.factor(cutree(hclust_ward, k=6))
```

In order to visualise the clusters, the *groups* object need to be appended onto *shan_sf* simple feature object.

```{r}
shan_sf_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER`=`as.matrix.groups.`)

qtm(shan_sf_cluster, "CLUSTER")
```

The choropleth map above reveals the clusters are very fragmented. The is one of the major limitation when non-spatial clustering algorithm such as hierarchical cluster analysis method is used.

# 7.0 Spatially Constrained Clustering

## 7.1 SKATER approach

### 7.1.1 Converting into SpatialPolygonsDataFrame

The SKATER function only support **sp** objects such as SpatialPolygonDataFrame. Thus, we need to convert shan_sf into SpatialPolygonDataFrame using the as_Spatial() function of sf package.

```{r}
shan_sp <- as_Spatial(shan_sf)
```

### 7.1.2 Computing Neighbour List

Next, we have to compute the neighbours list from polygon list.

```{r}
shan.nb <- poly2nb(shan_sp)
summary(shan.nb)
```

```{r}
plot(shan_sp, 
     border=grey(.5))
plot(shan.nb, 
     coordinates(shan_sp), 
     col="blue", 
     add=TRUE)
```

### 7.1.3 Computing minimum spanning tree

#### 7.1.3.1 Calculating edge costs

The [*nbcosts()*](https://r-spatial.github.io/spdep/reference/nbcosts.html) of **spdep** package is used to compute the cost of each edge. It measures the distance between its nodes.

```{r}
lcosts <- nbcosts(shan.nb, shan_ict)
```

```{r}
shan.w <- nb2listw(shan.nb, 
                   lcosts, 
                   style="B")
summary(shan.w)
```

To compute the minimum spanning tree:

```{r}
shan.mst <- mstree(shan.w)
dim(shan.mst)
```

::: callout-note
## Note

Note that the dimension is 54 and not 55. This is because the minimum spanning tree consists on n-1 edges (links) in order to traverse all the nodes.
:::

Let us take a look at the contents of `shan.mst`.

```{r}
head(shan.mst)
```

```{r}
plot(shan_sp, border=gray(.5))
plot.mst(shan.mst, 
         coordinates(shan_sp), 
         col="blue", 
         cex.lab=0.7, 
         cex.circles=0.005, 
         add=TRUE)
```

### 7.1.4 Computing spatially constrained clusters using SKATER method

```{r}
clust6 <- spdep::skater(edges = shan.mst[,1:2], 
                 data = shan_ict, 
                 method = "euclidean", 
                 ncuts = 5)
```

```{r}
str(clust6)
```

```{r}
plot(shan_sp, border=gray(.5))
plot(clust6, 
     coordinates(shan_sp), 
     cex.lab=.7,
     groups.colors=c("red","green","blue", "brown", "pink"),
     cex.circles=0.005, 
     add=TRUE)
```

### 7.1.5 Visualising the clusters in choropleth map

We can plot the newly derived clusters.

```{r}
groups_mat <- as.matrix(clust6$groups)
shan_sf_spatialcluster <- cbind(shan_sf_cluster, as.factor(groups_mat)) %>%
  rename(`SP_CLUSTER`=`as.factor.groups_mat.`)
qtm(shan_sf_spatialcluster, "SP_CLUSTER")

hclust.map <- qtm(shan_sf_cluster,
                  "CLUSTER") + 
  tm_borders(alpha = 0.5) 

shclust.map <- qtm(shan_sf_spatialcluster,
                   "SP_CLUSTER") + 
  tm_borders(alpha = 0.5) 

tmap_arrange(hclust.map, shclust.map,
             asp=NA, ncol=2)
```

## 7.2 ClustGeo Method

ClustGeo package provides function called `hclustgeo()` to perform a typical Ward-like hierarchical clustering just like `hclust()`.

### 7.2.1 Non-spatially constrained hierarchical clustering

To perform non-spatially constrained hierarchical clustering, we only need to provide the function a dissimilarity matrix.

```{r}
nongeo_cluster <- hclustgeo(proxmat)
plot(nongeo_cluster, cex = 0.5)
rect.hclust(nongeo_cluster, 
            k = 6, 
            border = 2:5)
```

To map out the clusters:

```{r}
groups <- as.factor(cutree(nongeo_cluster, k=6))
shan_sf_ngeo_cluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)
qtm(shan_sf_ngeo_cluster, "CLUSTER")
```

### 7.2.2 Spatially constrained hierarchical clustering

```{r}
dist <- st_distance(shan_sf, shan_sf)
distmat <- as.dist(dist)
```

The `choicealpha()` will be used to determine a suitable value for the mixing parameter alpha.

```{r}
cr <- choicealpha(proxmat, distmat, range.alpha = seq(0, 1, 0.1), K=6, graph = TRUE)
```

From the graph outputs, we will choose 0.3 as our alpha value.

```{r}
clustG <- hclustgeo(proxmat, distmat, alpha = 0.3)
groups <- as.factor(cutree(clustG, k=6))
shan_sf_Gcluster <- cbind(shan_sf, as.matrix(groups)) %>%
  rename(`CLUSTER` = `as.matrix.groups.`)
```

Let us plot the map of the newly delineated spatially constrained clusters.

```{r}
qtm(shan_sf_Gcluster, "CLUSTER")
```
