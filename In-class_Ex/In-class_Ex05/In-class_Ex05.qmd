---
title: "In-class Exercise 5: Global and Local Measures of Spatial Autocorrelation, Emerging Hot Spot Analysis: sfdep methods"
execute:
  eval: true
  echo: true
  warning: false
date: 02/05/2024
date-modified: last-modified
code-annotations: hover
---

**Loading packages**

```{r}
pacman::p_load(sf, tmap, sfdep, tidyverse)
```

**The Data**

For the purpose of this In-class exercise, the Hunan data sets will be used. There are two data sets used in this case, they are:

-   Hunan, a geospatial data set in ESRI shapefile format

-   Hunan_2012, an attribute data set in csv format

**Importing geospatial files**

```{r}
hunan <- st_read(dsn="data/geospatial",
                 layer="Hunan")
```

**Importing aspatial files**

```{r}
hunan_2012 <- read_csv("data/aspatial/Hunan_2012.csv")
```

```{r}
hunan_gdppc <- read_csv("data/aspatial/Hunan_GDPPC.csv")
```

Performing relational join on the two data frames - `hunan` and `hunan_2012`

```{r}
hunan_gdppc_joined <- left_join(hunan,hunan_2012) %>%
  select(1:4, 7, 15)
```

Visualizing the choropleth map

```{r}
tmap_mode('plot')

tm_shape(hunan_gdppc_joined) +
  tm_fill("GDPPC",
          style = "quantile",
          palette = "Blues",
          title = "GDPPC") +
  tm_layout(main.title = "Distribution of GDP per capita by county",
            main.title.position = "center",
            main.title.size = 1.2,
            legend.height = 0.45,
            legend.width = 0.35,
            frame = TRUE) + 
  tm_borders(alpha = 0.5) +
  tm_compass(type="8star", size = 2) +
  tm_scale_bar() +
  tm_grid(alpha =0.2)
  
```

**Step 1: Deriving contiguity weights: Queen's method**

```{r}
wm_q <- hunan_gdppc_joined %>%
  mutate(nb = st_contiguity(geometry),      # <1>
         wt = st_weights(nb,
                         style = "W"),
         .before = 1)                       # <2>
```

1.  We are using Queen's method as default. To use Rook's method, write `st_contiguity(geometry, queen = FALSE)`

2.  `.before = 1` will put the mutated columns in front of column 1

**Computing Global Moran's I**

```{r}
moran_i <- global_moran(wm_q$GDPPC,
                        wm_q$nb,
                        wm_q$wt)
```

**Performing Global Moran's I Permutation test**

It is always a good practice to use `set.seed()`.

```{r}
set.seed(1234)
```

`global_moran_perm()` is used to perform **Monte Carlo simulation**.

```{r}
global_moran_perm(wm_q$GDPPC,
                  wm_q$nb,
                  wm_q$wt,
                  nsim = 99)    # <1>
```

1.  `nsim = 99` will run **100 simulations** because the simulations will start from 0

# **Emerging Hot Spot Analysis: sfdep methods**

## **Overview**

Emerging Hot Spot Analysis (EHSA) is a spatio-temporal analysis method for revealing and describing how hot spot and cold spot areas evolve over time. The analysis consist of four main steps:

-   Building a space-time cube,

-   Calculating Getis-Ord local Gi\* statistic for each bin by using an FDR correction,

-   Evaluating these hot and cold spot trends by using Mann-Kendall trend test,

-   Categorising each study area location by referring to the resultant trend z-score and p-value for each location with data, and with the hot spot z-score and p-value for each bin.

## **Creating a Time Series Cube**

```{r}
GDPPC_st <- spacetime(hunan_gdppc, hunan,
                      .loc_col = "County",
                      .time_col = "Year")
```

```{r}
is_spacetime_cube(GDPPC_st)
```

## **Deriving the spatial weights**

```{r}
GDPPC_nb <- GDPPC_st %>%
  activate("geometry") %>%
  mutate(nb = include_self(st_contiguity(geometry)),
         wt = st_inverse_distance(nb, geometry,
                                  scale = 1,
                                  alpha = 1),
         .before = 1) %>%
  set_nbs("nb") %>%
  set_wts("wt")
```

## **Computing Gi\***

```{r}
gi_stars <- GDPPC_nb %>% 
  group_by(Year) %>% 
  mutate(gi_star = local_gstar_perm(
    GDPPC, nb, wt)) %>% 
  tidyr::unnest(gi_star)
```

## **Mann-Kendall Test**

```{r}

```
